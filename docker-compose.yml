version: '3.8'
services:
  #************* ----- caddy ----- ***********************
  caddy:
    container_name: ft_t_caddy
    image: caddy:latest
    volumes:
      - ./services/proxy/Caddyfile:/etc/caddy/Caddyfile
      - caddylog:/var/log/
    ports:
      - "8000:80"
      - "8443:443"
    # healthcheck:
    #   test: ["CMD", "curl", "-k", "-f", "https://localhost"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 30s
    depends_on:
      - auth-service
      - user-service
      - game-service
      - stats-service
      - frontend
    networks:
      - frontend
      - backend
    restart: always

  #************* ----- db ----- ***********************
  db:
    container_name: db
    build:
      context: ./services/DB
      dockerfile: Dockerfile
    networks:
      - backend
    env_file: .env
    volumes:
      - db_data:/var/lib/mysql
    restart: always
  
  #************* ----- Backend ----- ***********************
  auth-service:
    container_name: ft_t_auth
    build: 
      context: ./api/
      dockerfile: Dockerfile.dev
      args:
        - APP_DIR=auth-service
    volumes:
      -  ./api/auth-service:/app
      - /app/node_modules
   
    env_file: .env
    depends_on:
      - db
    networks:
      - backend
    restart: always

  user-service:
    container_name: ft_t_user
    build: 
      context: ./api/
      dockerfile: Dockerfile.dev
      args:
        - APP_DIR=user-service
    volumes:
      - ./api/user-service:/app
      - /app/node_modules
   
    env_file: .env 
    depends_on:
      - db
    networks:
      - backend
    restart: always

  game-service:
    container_name: ft_t_game
    build: 
      context: ./api/
      dockerfile: Dockerfile.dev
      args:
        - APP_DIR=game-service
    volumes:
      - ./api/game-service:/app
      - /app/node_modules
   
    env_file: .env
    depends_on:
      - db
    networks:
      - backend
    restart: always

  stats-service:
    container_name: ft_t_stats
    build: 
      context: ./api/
      dockerfile: Dockerfile.dev
      args: 
        - APP_DIR=stats-service
    volumes:
      - ./api/stats-service:/app
      - /app/node_modules
    
    env_file: .env
    depends_on:
      - db
    networks:
      - backend
    restart: always
  

# ************* ----- frontend ----- ***********************
  frontend:
    container_name: ft_t_frontend
    build: 
      context: ./frontend
      dockerfile: Dockerfile.dev
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
    networks:
      - frontend
    restart: always


#************* ----- ELK ----- ***********************

  # setup:
  #   build:
  #     context: ./services/ELK/setup
  #     dockerfile: Dockerfile
  #   container_name: setup
  #   depends_on:
  #     - caddy
  #   volumes:
  #     - certs:/usr/share/elasticsearch/config/certs
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   env_file:
  #     - .env
  #   networks:
  #     - elk
  #   restart: on-failure
  #   # restart: always
  #   healthcheck:
  #     test: ['CMD-SHELL', '[ -f /usr/share/elasticsearch/config/certs/elasticsearch/elasticsearch.crt ]']
  #     interval: 1s
  #     timeout: 5s
  #     retries: 120
  
  # elasticsearch:
  #   container_name: elasticsearch
  #   build:
  #     context: ./services/ELK/elasticsearch
  #     dockerfile: Dockerfile
  #   depends_on:
  #     setup:
  #       condition: service_healthy
  #   volumes:
  #     - certs:/usr/share/elasticsearch/config/certs
  #     - elasticsearch:/usr/share/elasticsearch/data
  #   ports:
  #     - '9200:9200'
  #   env_file:
  #     - .env
  #   networks:
  #     - elk
  #   restart: always
  #   mem_limit: ${MEM_LIMIT}
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   healthcheck:
  #     test: ['CMD-SHELL', 'curl -s -k https://elasticsearch:9200 | grep -q "missing authentication credentials"']
  #     interval: 5s
  #     timeout: 5s
  #     retries: 120
  
  # kibana:
  #   container_name: kibana
  #   build:
  #     context: ./services/ELK/kibana
  #     dockerfile: Dockerfile
  #   depends_on:
  #     setup:
  #       condition: service_healthy
  #   volumes:
  #     - certs:/usr/share/kibana/config/certs
  #     - kibana:/usr/share/kibana/data
  #   ports:
  #     - '5601:5601'
  #   env_file:
  #     - .env
  #   networks:
  #     - elk
  #   restart: always
  #   mem_limit: ${MEM_LIMIT}
  
  # logstash:
  #   container_name: logstash
  #   build:
  #     context: ./services/ELK/logstash
  #     dockerfile: Dockerfile
  #   depends_on:
  #     elasticsearch:
  #       condition: service_healthy
  #   volumes:
  #     - certs:/usr/share/logstash/config/certs
  #     - caddylog:/var/log/
  #   env_file:
  #     - .env
  #   networks:
  #     - elk
  #   restart: always
  
  #***************************----- grafana -----*******************************************
  
  # prometheus:
  #   container_name: ft_t_prometheus
  #   image: prom/prometheus
  #   volumes:
  #     - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  #   ports:
  #     - "9090:9090"

  # grafana:
  #   container_name: ft_t_grafana
  #   image: grafana/grafana
  #   ports:
  #     - "3000:3000"

volumes:
  db_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/volumes/data/

  certs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/volumes/certs/
  
  elasticsearch:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/volumes/es01/
  kibana:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/volumes/kibana/
  caddylog:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/volumes/caddylog/
  
  
networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  elk:
    driver: bridge

